{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exceptional-jewel",
   "metadata": {},
   "source": [
    "## Zadanie rekrutacyjne\n",
    "Stwórz model uczenia maszynowego ktory będzie rozpoznawał czy tekst napisany jest w języku polskim czy angielskim. Dane uczące pobierz z dowolnego źródła. Metoda uczenia także jest dowolna. Nie korzystaj z gotowych funkcji rozpoznających języki."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-salmon",
   "metadata": {},
   "source": [
    "W celu rozwiązania zadania pobrałem i przetworzyłem 2 książki \"Władca Pierścieni\" w wersji polskiej i angielskiej. Następnie za pomocą naiwnego klasyfikatora Bayesa stworzyłem model ropoznający język tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "obvious-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mental-observation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_book(book_path):\n",
    "    book = []\n",
    "    with pdfplumber.open(book_path) as pdf:\n",
    "        pages = pdf.pages\n",
    "        for i,pg in enumerate(pages):\n",
    "            try:\n",
    "                text = pages[i].extract_text()\n",
    "                text = text.strip()\n",
    "                book.append(text)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uwaga dużo obliczeń, jeśli nie chcesz czekać na wynik pomiń te komórkę\n",
    "\n",
    "book_pl = load_book('LOTR_pl_all.pdf')\n",
    "book_eng = load_book('LOTR_eng_all.pdf')\n",
    "\n",
    "# zapis książek jako plik .pickle\n",
    "with open(r'./pickle/book_pl.pickle', 'wb') as writer:\n",
    "    pickle.dump(book_pl, writer)\n",
    "with open( r'./pickle/book_eng.pickle', 'wb') as writer:\n",
    "    pickle.dump(book_eng, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytanie książek\n",
    "book_pl =  pickle.load(open(r'./pickle/book_pl.pickle', 'rb'))\n",
    "book_eng =  pickle.load(open(r'./pickle/book_eng.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-creature",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'ilość stron pl: {len(book_pl)} \\nilość stron eng: {len(book_eng)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_list(book): \n",
    "    \"\"\"Funkcja przetwarza wszystkie słowa z książki i dodaje je do jednej listy\"\"\"\n",
    "    words_all = []\n",
    "    for page in book:\n",
    "        for words_page in page.split():\n",
    "            words_all.append(words_page)\n",
    "    return words_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_all_pl = words_list(book_pl)\n",
    "words_all_eng = words_list(book_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heavy-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('pl:', len(words_all_pl), '\\neng:', len(words_all_eng)) #ilość słów"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-insured",
   "metadata": {},
   "source": [
    "Angielska wersja ma więcej słów (około 10 000)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-gather",
   "metadata": {},
   "source": [
    "Tworzę Data Frame, każdy rząd to jedno słowo z etykietą języka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pl = np.array(['pl' for i in range(len(words_all_pl))])\n",
    "label_eng = np.array(['eng' for i in range(len(words_all_eng))])\n",
    "df_pl = pd.DataFrame(list(zip(words_all_pl, label_pl)), columns=['word', 'label'])\n",
    "df_eng = pd.DataFrame(list(zip(words_all_eng, label_eng)), columns=['word', 'label'])\n",
    "df_all = pd.concat([df_pl, df_eng], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    \"\"\"operacje czyszczenia tekstu\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W', '', text) # usunięcie znaków np. nawiasy\n",
    "    text = re.sub(r'\\d*', '',  text) # usuniecie liczb\n",
    "    return text\n",
    "\n",
    "df_all['word'] = df_all['word'].apply(clean_data)\n",
    "df_all = df_all[df_all['word'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_all['word'], df_all['label'],\n",
    "                                                    test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-treasury",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wytrenowanie modelu\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-command",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-things",
   "metadata": {},
   "source": [
    "Model radzi sobie całkiem dobrze. Confusion matrix wskazuje ile wartości zostało wskazanych poprawnie lub nie. Spośród wszystkich angielskich słow około 113 000* słów angielskich zostało poprawnie wskazanych jako angielskie, a tylko około 1 400 jako polskie.  \n",
    "Trochę gorzej radzi sobie ze słowami polskimi (73 000 poprawnie, 16 000 nie). Model ma tendencję do wskazywania słów jako angielskie, gdy ma problem ze sklasyfikowaniem.  \n",
    "\n",
    "*w zależności od losowych procesów, przy każdym trenowaniu podane dane mogą się delikatnie różnić"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-canada",
   "metadata": {},
   "source": [
    "## Testowanie w praktyce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'We Francji dużo więcej zakażeń'\n",
    "pipeline.predict_proba(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-passion",
   "metadata": {},
   "source": [
    "Słowo \"We\" w 91,8 % jest angielskie, w 8.2 % polskie.  \n",
    "Słowo \"Francji\" w 56,1 % jest angielskie, w 43,9 % polskie.  \n",
    "Słowo \"Dużo\" w 1,58 % jest angielskie, w 98,41 % polskie.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-bachelor",
   "metadata": {},
   "source": [
    "Gdy model nie zna słowa zawsze podaje wartość 0.56 na angielski i 0.43 na polski. Być może jest to spowodowane większą ilością słów anglieskich w zbiorze treningowym. W funkcji poniżej zawarłem klauzulę, która pomija warotości w tym przedziale. Następnie dodaje prawdopodobieństwa poszczególnych słów, aby otrzymać całkowite prawdopodobieństwo języka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immune-pocket",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def language_detection(text):\n",
    "    eng = 0\n",
    "    pl = 0\n",
    "    count = 0\n",
    "    length = len(text.split())\n",
    "    \n",
    "    for i in range(length):\n",
    "        score_eng = pipeline.predict_proba(text.split())[i][0]\n",
    "        if score_eng < 0.57 and score_eng > 0.43:\n",
    "            score_eng = 0\n",
    "            count += 1\n",
    "        score_pl = pipeline.predict_proba(text.split())[i][1]\n",
    "        if score_pl < 0.57 and score_pl > 0.43:\n",
    "            score_pl = 0\n",
    "        eng += score_eng\n",
    "        pl += score_pl\n",
    "    \n",
    "    if count == length:\n",
    "        print('Did not recogize language')\n",
    "    else:\n",
    "        eng = round((eng / (length - count)) * 100, 2)\n",
    "        pl = round((pl / (length - count)) * 100, 2)\n",
    "        print(f'In {eng}% this is english \\nIn {pl}% this is polish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I read the news today, oh boy About a lucky man who made the grade And though the news was rather sad Well'\n",
    "language_detection(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = 'We Francji dużo więcej zakażeń, ale mniej zgonów niż w Polsce. Dlaczego mamy tak wysoką śmiertelność?'\n",
    "language_detection(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-living",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_3 = 'hello world, witaj świecie'\n",
    "language_detection(text_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_4 = 'Jakub'\n",
    "language_detection(text_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-granny",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
